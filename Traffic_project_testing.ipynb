{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdoGHd+4BD2yGiNnmNty3j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcusflygar1-hash/project_testing/blob/main/Traffic_project_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the neccesary libraries for the project.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import auc,roc_auc_score,roc_curve, accuracy_score, confusion_matrix, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, mean_squared_log_error, r2_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor"
      ],
      "metadata": {
        "id": "yO-vUlw1yKh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nya features att addera:\n",
        "\n",
        "Veckodagar, månad & ?\n",
        "\n",
        "\n",
        "Gör modellen mer robust, genom att normalisera portal datan.\n",
        "\n",
        "GridSearchCV? .. tar lång tid dock...\n",
        "\n"
      ],
      "metadata": {
        "id": "_BM5brUj9AIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data prepping"
      ],
      "metadata": {
        "id": "LHVSQYKfLwaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tds = pd.read_csv('training_dataset.csv', sep=';')\n",
        "ev_ds = pd.read_csv('evaluation_dataset.csv',sep=';')\n",
        "f_ev_ds = pd.read_csv('final_evaluation_dataset.csv', sep=';')\n"
      ],
      "metadata": {
        "id": "-rVQM72Kq4ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n training dataset')\n",
        "print(tds.head())\n",
        "print('\\n evaluation dataset------------')\n",
        "print(ev_ds.head())\n",
        "print('\\n final evaluation dataset --------')\n",
        "print(f_ev_ds.head())"
      ],
      "metadata": {
        "id": "ed8Fuwk6KpPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training dataset')\n",
        "print(tds.isna().sum())\n",
        "print('\\n eval dataset')\n",
        "print(ev_ds.isna().sum())\n",
        "print('\\n final eval dataset')\n",
        "print(f_ev_ds.isna().sum())"
      ],
      "metadata": {
        "id": "f4rMw4EXL1_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop all NaN values.\n",
        "f_ev_ds_noNAN = f_ev_ds.dropna()\n",
        "tds_noNAN = tds.dropna()\n",
        "ev_ds_noNAN = ev_ds.dropna()"
      ],
      "metadata": {
        "id": "vXWad_xvU9OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tds_noNAN.head(10)"
      ],
      "metadata": {
        "id": "aFr7yJIwOHBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter all of the datasets so we only have data from the correct portal / sensor group\n",
        "#This will ensure we can get more reliable data as the predicitons just do not work when they are all together.\n",
        "\n",
        "portal_of_interest = \"E4S 56,780\"\n",
        "tds_noNAN = tds_noNAN[tds_noNAN['PORTAL'] == portal_of_interest]\n",
        "ev_ds_noNAN = ev_ds_noNAN[ev_ds_noNAN['PORTAL'] == portal_of_interest]\n",
        "f_ev_ds_noNAN = f_ev_ds_noNAN[f_ev_ds_noNAN['PORTAL'] == portal_of_interest]\n",
        "print(f\"Training rows for {portal_of_interest}: {len(tds_noNAN)}\")\n",
        "print(f\"Evaluation rows for {portal_of_interest}: {len(ev_ds_noNAN)}\")"
      ],
      "metadata": {
        "id": "dZwd0l9GMwal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Org dataset amount:{len(tds)}')\n",
        "print(f'Org dataset amount:{len(ev_ds)}')\n",
        "print(f'Org dataset amount:{len(f_ev_ds)}')\n",
        "\n",
        "print(f'Dropped Dataset amount:{len(tds_noNAN)}')\n",
        "print(f'Dropped Dataset amount:{len(ev_ds_noNAN)}')\n",
        "print(f'Dropped Dataset amount:{len(f_ev_ds_noNAN)}')"
      ],
      "metadata": {
        "id": "o2L9iJoAU529"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forts."
      ],
      "metadata": {
        "id": "BRiwG8uAMl4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define what is congestion and what is not.\n",
        "# congestion_flow = tds[tds['FLOW'] < 100]\n",
        "# congestion_speed = tds[tds['SPEED_MS_AVG'] < 10]\n",
        "\n",
        "ev_ds_noNAN['datetime'] = pd.to_datetime(ev_ds_noNAN['Date'].astype(str) + ' ' + ev_ds_noNAN['Time'])\n",
        "\n",
        "ev_ds_offpeak = ev_ds_noNAN[(ev_ds_noNAN['datetime'].dt.time >= pd.to_datetime(\"04:00:00\").time()) &\n",
        "                            (ev_ds_noNAN['datetime'].dt.time <= pd.to_datetime(\"06:00:00\").time())]\n",
        "ev_ds_onpeak = ev_ds_noNAN[(ev_ds_noNAN['datetime'].dt.time >= pd.to_datetime(\"07:30:00\").time()) &\n",
        "                           (ev_ds_noNAN['datetime'].dt.time <= pd.to_datetime(\"08:30:00\").time())]\n",
        "\n",
        "ev_ds_offpeak = ev_ds_offpeak[(ev_ds_offpeak['FLOW'] > 0) &\n",
        "                              (ev_ds_offpeak['SPEED_MS_AVG'].between(5, 50))]\n",
        "ev_ds_onpeak = ev_ds_onpeak[(ev_ds_onpeak['FLOW'] > 0) &\n",
        "                            (ev_ds_onpeak['SPEED_MS_AVG'].between(5, 50))]\n"
      ],
      "metadata": {
        "id": "oezW0AffMMBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the date and time columns int o just one \"datetime\" coluinm\n",
        "tds_noNAN['datetime'] = pd.to_datetime(tds_noNAN['Date'].astype(str) + ' ' + tds_noNAN['Time'])\n",
        "\n",
        "# filtering the dataset to only include the data within hours 04:00 and 06:00, this to get an accurate description of free flow speed.\n",
        "tds_offpeak = tds_noNAN[(tds_noNAN['datetime'].dt.time >= pd.to_datetime(\"04:00:00\").time()) &\n",
        "                (tds_noNAN['datetime'].dt.time <= pd.to_datetime(\"06:00:00\").time())]\n",
        "\n",
        "tds_onpeak= tds_noNAN[(tds_noNAN['datetime'].dt.time >= pd.to_datetime(\"07:30:00\").time()) &\n",
        "                  (tds_noNAN['datetime'].dt.time <= pd.to_datetime(\"08:30:00\").time())]\n",
        "#Ensuring we only get correctly read speeds. E.g Removing any negative speeds and random slow drivers etc\n",
        "#that do not actually depict the actual free flowspeed e.g people speeding and drinving super slow...\n",
        "tds_offpeak = tds_offpeak[(tds_offpeak['FLOW'] > 0) &\n",
        "                        (tds_offpeak['SPEED_MS_AVG'].between(2, 50))]\n",
        "tds_onpeak = tds_onpeak[(tds_onpeak['FLOW'] > 0) &\n",
        "                        (tds_onpeak['SPEED_MS_AVG'].between(2, 50))]"
      ],
      "metadata": {
        "id": "Ou3DA0M5VJKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining what congestion is for off peak hours\n",
        "ffs_kmh = 85 #Through basic theory, a 4 lane road with a speed limit of 70 km/h, would have the free flow speed around 85 km/h\n",
        "ffs_ms = ffs_kmh * 0.277778 # Converting km/h to m/s\n",
        "cong_th = (ffs_kmh * 0.7) # We use mild congestion as a staple, as otherwise the dataset for congested times is just to small to work with.\n",
        "print(f\"Free flow speed in [km/h]: {ffs_kmh}\")\n",
        "print(f\"Free flow speed in [m/s]: {ffs_ms}\")\n",
        "print(f\"Congestion Speed Threshold [km/h]: {cong_th}\")\n",
        "\n",
        "#hard coding cong as 60 [km/h] for simplicty\n",
        "cong_th_2 = 60"
      ],
      "metadata": {
        "id": "pqte_MKeeSJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining what congestion is for on peak hours\n",
        "ffs_kmh_2 = 85 #Through basic theory, a 4 lane road with a speed limit of 70 km/h, would have the free flow speed around 85 km/h\n",
        "ffs_ms_2 = ffs_kmh_2 * 0.277778 # Converting km/h to m/s\n",
        "cong_th_2 = (ffs_kmh_2 * 0.7) # We use mild congestion as a staple, as otherwise the dataset for congested times is just to small to work with.\n",
        "print(f\"Free flow speed in [km/h]: {ffs_kmh_2}\")\n",
        "print(f\"Free flow speed in [m/s]: {ffs_ms_2}\")\n",
        "print(f\"Congestion Speed Threshold [km/h]: {cong_th_2}\")\n",
        "\n",
        "#hard coding cong as 61 [km/h] for simplicty and to catch those going around 60km/h for a fairer comparison\n",
        "cong_th_2 = 61"
      ],
      "metadata": {
        "id": "ArMy7NEYcvl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Distribution of speeds during on peak hours\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(tds_onpeak['SPEED_MS_AVG']*3.6, bins=40, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "\n",
        "plt.axvline(ffs_kmh_2, color='red', linestyle='--', linewidth=2, label=f'85th percentile (FFS = {ffs_kmh_2:.2f} km/h)')\n",
        "plt.axvline(cong_th_2, color='green', linestyle='--', linewidth=2, label=f'Congestion threshold (70% of FFS = {cong_th_2:.2f} km/h)')\n",
        "plt.xlabel(\"Speed (km/h)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Speeds During On-Peak (07:30–08:30)\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ak48AujNQBWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dist of speeds during off peak hours.\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(tds_offpeak['SPEED_MS_AVG']*3.6, bins=40, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "plt.axvline(ffs_kmh, color='red', linestyle='--', linewidth=2, label=f'85th percentile (FFS = {ffs_kmh:.2f} km/h)')\n",
        "plt.axvline(cong_th, color='green', linestyle='--', linewidth=2, label=f'Congestion threshold (70% of FFS = {cong_th_2:.2f} km/h)')\n",
        "\n",
        "\n",
        "\n",
        "plt.xlabel(\"Speed (km/h)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Speeds During Off-Peak (04:00–06:00)\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6uBlKaW0X-2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cong_th_2)\n",
        "tds_onpeak.info()"
      ],
      "metadata": {
        "id": "R5ParPnNbdFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creat our rolling and shifting.\n",
        "tds_onpeak['pred15'] = tds_onpeak['SPEED_MS_AVG'].shift(-1).rolling(15).mean()\n",
        "tds_onpeak['congestion'] = (tds_onpeak['pred15']*3.6 <= cong_th_2).astype(int)"
      ],
      "metadata": {
        "id": "qScNu1yw0KIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tds_onpeak[\"speed_norm\"] = (\n",
        "    tds_onpeak.groupby(\"DP_ID\")[\"SPEED_MS_AVG\"]\n",
        "    .transform(lambda x: (x - x.mean()) / x.std())\n",
        ")\n",
        "\n",
        "ev_ds_onpeak[\"speed_norm\"] = (\n",
        "    ev_ds_onpeak.groupby(\"DP_ID\")[\"SPEED_MS_AVG\"]\n",
        "    .transform(lambda x: (x - x.mean()) / x.std())\n",
        ")"
      ],
      "metadata": {
        "id": "jGTgbppNBiE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_features_targets(df, horizon=15, cong_th_2=50):\n",
        "    df = df.copy().sort_values(\"datetime\")\n",
        "\n",
        "    # rolling(horizon).mean() looks backward, so there fore shift looks *backwards* by 'horizon' to make it look intothe future\n",
        "    df[f'future_speed_{horizon}m'] = (\n",
        "        df['SPEED_MS_AVG']\n",
        "        .rolling(window=horizon, closed='right')\n",
        "        .mean()\n",
        "        .shift(-horizon) * 3.6\n",
        "    )\n",
        "\n",
        "    # sets a target where if future speed is less than congestion speed.\n",
        "    df['future_congestion'] = (df[f'future_speed_{horizon}m'] <= cong_th_2).astype(int)\n",
        "\n",
        "    # historical 15 minutes of data\n",
        "    df['past15_flow_mean']  = df['FLOW'].rolling(15).mean()\n",
        "    df['past15_flow_std']   = df['FLOW'].rolling(15).std()\n",
        "    df['past15_flow_min']   = df['FLOW'].rolling(15).min()\n",
        "    df['past15_flow_max']   = df['FLOW'].rolling(15).max()\n",
        "    df['past15_flow_slope'] = df['FLOW'] - df['FLOW'].shift(15)\n",
        "\n",
        "    df['past15_speed_mean']  = df['SPEED_MS_AVG'].rolling(15).mean()\n",
        "    df['past15_speed_std']   = df['SPEED_MS_AVG'].rolling(15).std()\n",
        "    df['past15_speed_min']   = df['SPEED_MS_AVG'].rolling(15).min()\n",
        "    df['past15_speed_max']   = df['SPEED_MS_AVG'].rolling(15).max()\n",
        "    df['past15_speed_slope'] = df['SPEED_MS_AVG'] - df['SPEED_MS_AVG'].shift(15)\n",
        "    df['past15_speed_norm_mean'] = df['speed_norm'].rolling(15).mean()\n",
        "    df['past15_speed_norm_std']  = df['speed_norm'].rolling(15).std()\n",
        "\n",
        "\n",
        "    #  trend features\n",
        "    df[\"flow_delta_5_15\"]  = df[\"FLOW\"].rolling(5).mean() - df[\"FLOW\"].rolling(15).mean()\n",
        "    df[\"speed_delta_5_15\"] = df[\"SPEED_MS_AVG\"].rolling(5).mean() - df[\"SPEED_MS_AVG\"].rolling(15).mean()\n",
        "    df[\"speed_diff_1\"] = df[\"SPEED_MS_AVG\"].diff(1)\n",
        "    df[\"flow_diff_1\"]  = df[\"FLOW\"].diff(1)\n",
        "    df[\"speed_range_15\"] = df[\"SPEED_MS_AVG\"].rolling(15).max() - df[\"SPEED_MS_AVG\"].rolling(15).min()\n",
        "    df[\"speed_ratio_15\"] = df[\"SPEED_MS_AVG\"].rolling(15).min() / df[\"SPEED_MS_AVG\"].rolling(15).max()\n",
        "\n",
        "\n",
        "    # time related features.\n",
        "    df[\"minute_of_day\"] = df[\"datetime\"].dt.hour * 60 + df[\"datetime\"].dt.minute\n",
        "    df[\"tod_sin\"] = np.sin(2 * np.pi * df[\"minute_of_day\"] / 1440)\n",
        "    df[\"tod_cos\"] = np.cos(2 * np.pi * df[\"minute_of_day\"] / 1440)\n",
        "    df[\"hour\"] = df[\"datetime\"].dt.hour\n",
        "    df[\"dayofweek\"] = df[\"datetime\"].dt.dayofweek\n",
        "    df[\"is_weekend\"] = (df[\"dayofweek\"] >= 5).astype(int)\n",
        "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
        "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
        "\n",
        "\n",
        "    # full feature list\n",
        "    features = [\n",
        "        'past15_flow_mean','past15_flow_std','past15_flow_min','past15_flow_max','past15_flow_slope',\n",
        "        'past15_speed_mean','past15_speed_std','past15_speed_min','past15_speed_max','past15_speed_slope',\n",
        "        'tod_sin','tod_cos'\n",
        "        ]\n",
        "    #target \"variable\"\n",
        "    target = 'future_congestion'\n",
        "\n",
        "    # dropping NaN values if they have appeared due to rolling and shift.\n",
        "    df = df.dropna(subset=features + [target])\n",
        "\n",
        "    return df, features, target\n"
      ],
      "metadata": {
        "id": "8bbb3AOOo8go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tds_fixad, features, target = make_features_targets(tds_onpeak, horizon=15, cong_th_2=cong_th_2)\n",
        "\n",
        "# 2. Create evaluation data (for 2022)\n",
        "ev_fixad, _, _ = make_features_targets(tds_onpeak, horizon=15)\n"
      ],
      "metadata": {
        "id": "5pCZiowXHPdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dela upp i train test splits."
      ],
      "metadata": {
        "id": "7L7zGdNjSYbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split into train / test sets\n",
        "X = tds_fixad[features].values\n",
        "y = tds_fixad[target].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
        "#Scaling the datat.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "wsZWjSzpKOQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test data models"
      ],
      "metadata": {
        "id": "qjbV9vQfLA_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#perform linear regression model.\n",
        "lr = LogisticRegression(random_state = 42, max_iter = 1000, class_weight='balanced')\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "y_pred_lr = lr.predict(X_test_scaled)\n",
        "y_prob_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"Logistic Regression Dataset\")\n",
        "print(confusion_matrix(y_test, y_pred_lr))\n",
        "print(classification_report(y_test, y_pred_lr, digits=3))\n",
        "print(f\" The roc_auc score is: {roc_auc_score(y_test, y_prob_lr)}\")"
      ],
      "metadata": {
        "id": "7ftEGsokKqdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0dd6e72"
      },
      "source": [
        "print(\"Class distribution in y_train:\")\n",
        "print(pd.Series(y_train).value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c060018"
      },
      "source": [
        "print(\"Class distribution in tds_fixad['future_congestion']:\")\n",
        "print(tds_fixad['future_congestion'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train)\n",
        "xgb_clf = XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=8,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "xgb_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict.\n",
        "y_pred_xgb = xgb_clf.predict(X_test_scaled)\n",
        "y_prob_xgb = xgb_clf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "\n",
        "# stats\n",
        "print(\"XGBoost Training dataset\")\n",
        "print(confusion_matrix(y_test, y_pred_xgb))\n",
        "print(classification_report(y_test, y_pred_xgb, digits=3))"
      ],
      "metadata": {
        "id": "EgkS9CZMaTzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=12,\n",
        "    class_weight='balanced',  # handle imbalance\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_clf.fit(X_train_scaled, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test_scaled)\n",
        "#stats\n",
        "print(\"Random Forest Training dataset\")\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf, digits=3))"
      ],
      "metadata": {
        "id": "z0FnSQBggD_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Logistic Regression Dataset\")\n",
        "print(confusion_matrix(y_test, y_pred_lr))\n",
        "print(classification_report(y_test, y_pred_lr, digits=3))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"XGBoost Training dataset\")\n",
        "print(confusion_matrix(y_test, y_pred_xgb))\n",
        "print(classification_report(y_test, y_pred_xgb, digits=3))\n",
        "print(\"\\n\")\n",
        "print(\"Random Forest Training dataset\")\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf, digits=3))\n",
        "\n"
      ],
      "metadata": {
        "id": "Tw46yVd8pU2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create empty list to store results\n",
        "results = []\n",
        "\n",
        "#for loop that adds results to the list \"results\"\n",
        "for name, pred in [\n",
        "    (\"Logistic Regression\", y_pred_lr),\n",
        "    (\"Random Forest\", y_pred_rf),\n",
        "    (\"XGBoost\", y_pred_xgb)\n",
        "]:\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_test, pred),\n",
        "        \"Precision\": precision_score(y_test, pred),\n",
        "        \"Recall\": recall_score(y_test, pred),\n",
        "        \"F1\": f1_score(y_test, pred)\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "#print the results.\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "-3eeZRo2gZLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation data models"
      ],
      "metadata": {
        "id": "MSdFxjEtpMlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ev_fixad, _, _ = make_features_targets(ev_ds_onpeak, horizon=15, cong_th_2=cong_th_2)"
      ],
      "metadata": {
        "id": "jk-WpHX1pPPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_eval = ev_fixad[features].values\n",
        "y_eval = ev_fixad[target].values\n",
        "X_eval_scaled = scaler.transform(X_eval)"
      ],
      "metadata": {
        "id": "LznoD6WWpUNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loop through models on evaluation dataset.\n",
        "for name, model in [\n",
        "    (\"Logistic Regression\", lr),\n",
        "    (\"Random Forest\", rf_clf),\n",
        "    (\"XGBoost\", xgb_clf)\n",
        "]:\n",
        "    y_pred_eval = model.predict(X_eval_scaled)\n",
        "    print(f\"\\n{name} (Evaluation 2022 dataset):\")\n",
        "    print(confusion_matrix(y_eval, y_pred_eval))\n",
        "    print(classification_report(y_eval, y_pred_eval, digits=3))"
      ],
      "metadata": {
        "id": "KKgaNypCpXxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWgi29FgpbbJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}